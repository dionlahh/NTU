{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG2yi0u-pHrD"
   },
   "source": [
    "# Main Question: \n",
    "\n",
    "### Can we predict performance of a given movie?\n",
    "\n",
    "In this project, we will be trying to predict the **performance** of a movie with help from various datasets. We hope to extract, clean and perform data expoloration on our gathered dataset to gain insights regarding the main question. We will then use a machine learning model to answer our question.\n",
    "\n",
    "\n",
    "Before we start, we determined a movie's **performance** based on it's movie **ratings** (given by critics), as well as the amount of **revenue** and **profits** it earned through the cinemas and theaters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEeIQd9QpHrI"
   },
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGZyjlk5pHrI"
   },
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6htcxo40pHrJ"
   },
   "source": [
    "# Data Acquisition\n",
    "We shall use movie data from TMBD and OMBD. Why scrape data from two sources?\n",
    "Firstly, we might uncover more variables or factors that might be useful to us in analysis of the performance. Secondly, there are many different variables for the same performance metric that we can obtain from OMBD. For instance, the rating of a movie can be measured by rotten tomatoes score, IMBD rating, or even both. To measure the profits of a movie, we can consider using box-office values or the budget variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRTgqOU4pHrJ"
   },
   "outputs": [],
   "source": [
    "#Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQReVlNUpHrJ"
   },
   "source": [
    "**NOTE**: Our API key has expired, thus running the code in this notebook will give an error. However, we have already saved the required data into multiple csv files, such as Bigger_Data.csv and cleanData.csv, which are provided in the same zip submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04-WeRjxpHrK"
   },
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BDAdz_YpHrK"
   },
   "source": [
    "# TMBD Dataset Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcwTuin0pHrK"
   },
   "outputs": [],
   "source": [
    "movielist=[]\n",
    "for i in range(9000):\n",
    "    url=\"https://api.themoviedb.org/3/discover/movie?api_key=4568df78fb309238e68974a24013b626&language=en-US&include_adult=false&page=\"+str(i)\n",
    "    #Note that we decided to exclude adult movies\n",
    "    r = requests.get(url)\n",
    "    if r.status_code in range(200,299): #if status code is in range(200,299), this indicates a successful request, unlike error 401 or 500s which are connection errors.\n",
    "        data = r.json()  #json form\n",
    "        results = data[\"results\"]\n",
    "        for j in range(len(results)):\n",
    "            movielist.append(results[j])\n",
    "   \n",
    "    \n",
    "             \n",
    "\n",
    "movieData = pd.DataFrame(movielist) #Convert raw JSON data in list to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR6negQJpHrK"
   },
   "source": [
    "We took into account the fact that children and teenagers are not able to watch adult movies, resulting in certain performance metrics, such as revenue or profits, to differ largely for adult movies and non adult movies. Therefore, we decided to exclude adult movies from our data sraping.\n",
    "\n",
    "\n",
    "This also applies for the language of a movie. English movies, being the global language, should be available to almost every country and audience. There should be a more even distribution of audience size, and thus perfomrance metrics would be less \"skewed\". This will make our predictions more fair.\n",
    "\n",
    "Thus, in our data extraction, we decided to include only english and non-adult movies, with use of parameters &language=en-US \n",
    "and include_adult=false in the URL search key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut8AfVTfpHrL",
    "outputId": "3165b9f2-c197-4013-b0f5-8a96950e3692"
   },
   "outputs": [],
   "source": [
    "movieData = pd.read_csv(\"Bigger_Data.csv\")\n",
    "\n",
    "movieData=movieData[movieData['imdb_id'].notnull()] #dropping all the null IMBD ID values from the TMBD set, so that we can scrape from OMBD\n",
    "\n",
    "print(len(movieData[\"imdb_id\"])) #So that we can know how many movies we can extract from OMBD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hT9-TbwpHrL"
   },
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEgDNroypHrL"
   },
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy51dwJRpHrL"
   },
   "source": [
    "# OMBD Dataset Acquistion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN_JebbOpHrL"
   },
   "outputs": [],
   "source": [
    "headers = {'user-agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36\"\n",
    "} #Set user agent, so that our server appears to be a human requesting for access to the website. Certain websites\n",
    "#might block what seems to be an automated response.\n",
    "\n",
    "ombdList=[]\n",
    "for i in range(1000): #OMBD API Keys have a limit of 1000 per day\n",
    "    url= \"http://www.omdbapi.com/?i=\" + str(movieData[\"imdb_id\"].values[i]) +\"&apikey=f625be79\"\n",
    "    r = requests.get(url,headers=headers)\n",
    "    if r.status_code in range(200,299):   #successful GET request\n",
    "       data = r.json()                    \n",
    "       ombdList.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ie93x47kpHrM"
   },
   "outputs": [],
   "source": [
    "for i in range(1000,2000):\n",
    "    url= \"http://www.omdbapi.com/?i=\" + str(movieData[\"imdb_id\"].values[i]) +\"&apikey=76052a79\"\n",
    "    r = requests.get(url,headers=headers)\n",
    "    if r.status_code in range(200,299):   #successful GET request\n",
    "       data = r.json() \n",
    "       ombdList.append(data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nSOXtpHpHrM"
   },
   "outputs": [],
   "source": [
    "for i in range(2000,3000):\n",
    "    url= \"http://www.omdbapi.com/?i=\" + str(movieData[\"imdb_id\"].values[i]) +\"&apikey=5018e12e\"\n",
    "    r = requests.get(url,headers=headers)\n",
    "    if r.status_code in range(200,299):   #successful GET request\n",
    "       data = r.json() \n",
    "       ombdList.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nY8FDr-epHrM"
   },
   "outputs": [],
   "source": [
    "for i in range(3000,4000):\n",
    "    url= \"http://www.omdbapi.com/?i=\" + str(movieData[\"imdb_id\"].values[i]) +\"&apikey=7b1bb186\"\n",
    "    r = requests.get(url,headers=headers)\n",
    "    if r.status_code in range(200,299):    #successful GET request\n",
    "       data = r.json() \n",
    "       ombdList.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTlteZvqpHrM"
   },
   "outputs": [],
   "source": [
    "ombdData=pd.DataFrame(ombdList)  #Convert raw JSON data in list to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjUCSNn8pHrN"
   },
   "source": [
    "---\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axST4E2tpHrN"
   },
   "outputs": [],
   "source": [
    "ombdData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ9G3VahpHrN"
   },
   "source": [
    "As we mentioned above, we judge the performance of a movie based on three seperate metrics, \"Ratings\", \"Revenue\" and \"Profits\".\n",
    "\n",
    "In order to increase the accuracy and reliability of the ratings of a movie, we made use of more than 1 rating metrics. Instead of relying solely on TMBD's popularity rating, we scrape OMDB datasets for other metrics such as IMBD ratings and Metascore.\n",
    "\n",
    "Other factors that might influence the profit or ratings of movies could be the data of its release and Genre.\n",
    "\n",
    "\n",
    "We also took OMBD Genres instead of using TMBD's genre list, as the TMBD genre list only contains a dictionary of genre IDs, while OMBD contains a string. As such, taking OMBD's genre column would mean that we do not have to data scrape again.\n",
    "E.g. \"Genre\":\"Action, Sci-Fi, Thriller\" (OMBD) vs {ID: 4; ID; 3} (IMBD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJwpxFq5pHrN"
   },
   "outputs": [],
   "source": [
    "ombdData = ombdData[[\"imdbID\",\"Metascore\", \"imdbRating\", \"Genre\", \"Year\", \"Ratings\", \"Rated\"]]\n",
    "#Merging the common rows of TMBD and OMBD dataset based on imbd ID\n",
    "CleaningData = pd.merge(movieData, ombdData, left_on = 'imdb_id', right_on = 'imdbID', how = 'right')\n",
    "CleaningData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dropping uncessary columns\n",
    "Dropping uncessary columns which obviously do not have any influence on profit or ratings.\n",
    "Some of these ratings are also caterogical, and are in the form of dictionaries, for example, actors, too many exist for us\n",
    "#to do analysis on this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFmv0tIApHrO"
   },
   "outputs": [],
   "source": [
    "to_drop = ['adult', 'backdrop_path', 'belongs_to_collection', 'genres', 'homepage', 'original_title','overview',\n",
    "           'poster_path','status','video', 'Unnamed: 0', \"id\", \"tagline\", \"spoken_languages\", \"production_companies\", \n",
    "           \"production_countries\", \"imdb_id\", \"imdbID\"] \n",
    "\n",
    "CleaningData.drop(to_drop, inplace=True, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dropping null, N/A and zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCO7VKmNpHrO"
   },
   "outputs": [],
   "source": [
    "CleaningData.isnull().sum() #Checking number of null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPd9nc-qpHrO"
   },
   "outputs": [],
   "source": [
    "for col in CleaningData.columns:\n",
    "  CleaningData = CleaningData[CleaningData[str(col)]!=\"N/A\"] #Some of the null values are labelled N/A\n",
    "\n",
    "for col in CleaningData.columns:\n",
    "  CleaningData = CleaningData[CleaningData[str(col)]!=0] #Taking out the null values that have a value of 0\n",
    "\n",
    "CleaningData.reset_index(drop=True, inplace=True) #Resetting the index (as taking rows out do not realign the index)\n",
    "\n",
    "for col in CleaningData.columns:\n",
    "  CleaningData[str(col)].fillna(value = \"Nan\", inplace = True) #filling NA values with NaN\n",
    "  CleaningData.drop(CleaningData[CleaningData[str(col)] == 'Nan' ].index , inplace=True) #Dropping the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgmm0W4mpHrO"
   },
   "outputs": [],
   "source": [
    "CleaningData.isnull().sum() #Checking number of null values in each column is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhxGdqEipHrP"
   },
   "outputs": [],
   "source": [
    "CleaningData.to_csv('cleanData.csv')  #This is our cleaned data without null values and uncessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data Extraction and Cleaning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
